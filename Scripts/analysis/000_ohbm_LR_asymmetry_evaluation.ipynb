{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import pingouin as pg \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evaluated bundles: 39\n"
     ]
    }
   ],
   "source": [
    "# read in the metric files saved as csv\n",
    "controls = pd.read_csv('../../DerivedData/extracted_diffusion_metrics_control_group.csv', index_col=0)\n",
    "preterms = pd.read_csv('../../DerivedData/extracted_diffusion_metrics_preterm_group.csv', index_col=0)\n",
    "\n",
    "### first get all regions pairs, metrics to be evaluated \n",
    "tract_names = np.unique(np.array([tract.split('_')[0] for tract in controls.columns[2:]]))\n",
    "print('Number of evaluated bundles: {}'.format(len(tract_names)))\n",
    "metrics = np.unique(np.array([tract.partition('_')[-1] for tract in controls.columns[2:]]))\n",
    "\n",
    "### create pairing - as there is fewer preterms, use their IDs to find matches with controls \n",
    "matched = pd.read_csv('../../DerivedData/subject_matching.csv', index_col=0)\n",
    "matched = matched[matched['preterm_ID'].isin(preterms['subject_id'].values)]\n",
    "\n",
    "controls = controls[controls.subject_id.isin(matched.matched_ID_with_outcome.values)]\n",
    "\n",
    "df = pd.concat([preterms, controls])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform paired t-test adjusted for multiple comparison to determine whether LR asymmetries exist per bundle/metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [['M1L-Brainstem', 'M1R-Brainstem'],\n",
    "        ['M1L-CaudL', 'M1R-CaudR'], \n",
    "        ['M1L-LentiL', 'M1R-LentiR'],\n",
    "        ['M1L-ParacentralL', 'M1R-ParacentralR'],\n",
    "        ['M1L-SubthalL', 'M1R-SubthalR',], \n",
    "        ['M1L-ThalfusL', 'M1R-ThalfusR'], \n",
    "         ['ParacentralL-Brainstem', 'ParacentralR-Brainstem'],\n",
    "         ['ParacentralL-CaudL', 'ParacentralR-CaudR'],\n",
    "         ['ParacentralL-LentiL', 'ParacentralR-LentiR'],\n",
    "         ['ParacentralL-SubthalL', 'ParacentralR-SubthalR'],\n",
    "         ['ParacentralL-ThalfusL', 'ParacentralR-ThalfusR'],\n",
    "         ['S1L-Brainstem', 'S1R-Brainstem'],\n",
    "         ['S1L-CaudL', 'S1R-CaudR'], \n",
    "        ['S1L-LentiL', 'S1R-LentiR'],\n",
    "        ['S1L-ParacentralL', 'S1R-ParacentralR'],\n",
    "        ['S1L-SubthalL', 'S1R-SubthalR',], \n",
    "        ['S1L-ThalfusL', 'S1R-ThalfusR'], \n",
    "         ['S1L-M1L', 'S1R-M1R'],\n",
    "         ['S1L-ParacentralL', 'S1R-ParacentralR']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "as_results = pd.DataFrame()\n",
    "\n",
    "i = 0\n",
    "for metric in metrics:\n",
    "    for pair in pairs:\n",
    "    \n",
    "        col1 = pair[0]+'_'+metric\n",
    "        col2 = pair[1]+'_'+metric\n",
    "        #print(col1, col2)\n",
    "        test = pg.ttest(x = df[col1].values, y=df[col2].values, paired=True)\n",
    "        \n",
    "        test[ 'region1'] = pair[0]\n",
    "        test['region2'] = pair[1]\n",
    "        test['metric'] = metric\n",
    "        test['p-val'] = \"{:.7f}\".format(test['p-val'].values[0])\n",
    "        \n",
    "        test['region_1_mean'] = np.mean(df[col1].values)\n",
    "        test['region_1_std'] = np.std(df[col1].values)\n",
    "        \n",
    "        test['region_2_mean'] = np.mean(df[col2].values)\n",
    "        test['region_2_std'] = np.std(df[col2].values)\n",
    "        \n",
    "        \n",
    "        as_results = as_results.append(test)\n",
    "\n",
    "as_results['p-val'] = as_results['p-val'].astype(np.float64) \n",
    "as_results['p-val'] = np.round(as_results['p-val'],6)\n",
    "\n",
    "reject, pvals_corr = pg.multicomp(as_results['p-val'].values, method='fdr_bh')\n",
    "as_results['p-val_fdr_corrected'] = pvals_corr\n",
    "\n",
    "reject, pvals_corr = pg.multicomp(as_results['p-val'].values, method='bonf')\n",
    "as_results['p-val_bonf_corrected'] = pvals_corr\n",
    "\n",
    "reject, pvals_corr = pg.multicomp(as_results['p-val'].values, method='holm')\n",
    "as_results['p-val_holm_corrected'] = pvals_corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "as_results.to_csv('../../Results/LR_diffusion_metric_asymmetries_stats.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NODDI_results = pd.DataFrame()\n",
    "\n",
    "## sanity check for pre and post processing NODDI \n",
    "for tract in tract_names:\n",
    "    NDI = ['NDI_post', 'NDI_pre']\n",
    "    ODI = ['ODI_post', 'ODI_pre']\n",
    "    \n",
    "    for metric in [NDI, ODI]:\n",
    "        col1 = tract+'_'+metric[1]\n",
    "        col2 = tract+'_'+metric[0]\n",
    "        \n",
    "        test = pg.ttest(x = df[col1].values, y=df[col2].values, paired=True)\n",
    "        \n",
    "        test[ 'region1'] = col1\n",
    "        test['region2'] = col2\n",
    "        \n",
    "        test['p-val'] = \"{:.7f}\".format(test['p-val'].values[0])\n",
    "        \n",
    "        test['region_1_mean'] = np.mean(df[col1].values)\n",
    "        test['region_1_std'] = np.std(df[col1].values)\n",
    "        \n",
    "        test['region_2_mean'] = np.mean(df[col2].values)\n",
    "        test['region_2_std'] = np.std(df[col2].values)\n",
    "        \n",
    "        NODDI_results = NODDI_results.append(test)\n",
    "\n",
    "NODDI_results['p-val'] = NODDI_results['p-val'].astype(np.float64) \n",
    "NODDI_results['p-val'] = np.round(NODDI_results['p-val'],6)\n",
    "\n",
    "reject, pvals_corr = pg.multicomp(NODDI_results['p-val'].values, method='fdr_bh')\n",
    "NODDI_results['p-val_fdr_corrected'] = pvals_corr\n",
    "\n",
    "reject, pvals_corr = pg.multicomp(NODDI_results['p-val'].values, method='bonf')\n",
    "NODDI_results['p-val_bonf_corrected'] = pvals_corr\n",
    "\n",
    "reject, pvals_corr = pg.multicomp(NODDI_results['p-val'].values, method='holm')\n",
    "NODDI_results['p-val_holm_corrected'] = pvals_corr        \n",
    "        \n",
    "NODDI_results.to_csv('../../Results/comparison_of_bundle_NODDI_metrics_pre_post_processing.csv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
